{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text-based Chatbot using LSTM (College)\n"
      ],
      "metadata": {
        "id": "Vz_VDEdJmwoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique: LSTM or Long Short Term Memory networks are an extension for Recurrent Neural Networks with explicitly extended memory capability well suited to handle long term dependencies. In the domain of chatbots for time series conversations, LSTM is shown to perform well and maintain the context for longer durations.\n",
        "\n",
        "\n",
        "Purpose: Answer the Frequently Asked Questions (FAQs) by prospective college students during out-of-hours.\n",
        "\n",
        "Benefit: The students tend to connect to the college's peer mentors in their convenience. If the students are coming from different parts of the world, the difference in time zones creates a problem for both the students and the mentors. Hence, the development of a chatbot in taking questions offline is imperative in solving these issues.\n",
        "\n",
        "How it works: LSTM-based neural network is trained on\n",
        "the dataset. The questions are tagged to the extracted data. This creates a question-answer dataset for training the chatbot.  \n",
        "\n",
        "Results: The results yielded a high accuracy score, inline with the correctness of the chatbot in answering the queries.\n",
        "\n",
        "Caveat: More dataset could have been included if we were given a longer time."
      ],
      "metadata": {
        "id": "Jb8Ju40vmqkl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLmkz-kFPmMQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# ! pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
        "import pickle\n",
        "import re\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "NI3eW-Hyvx0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive in Colab\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "wIwJ4V0asim2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32004487-f430-4dcf-d42d-e3cbaf5a9034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open(r'/content/gdrive/MyDrive/course_bachelors.yaml') as file:\n",
        "  documents = yaml.load(file, Loader=yaml.FullLoader)\n",
        "documents"
      ],
      "metadata": {
        "id": "wKO4UEtpnLki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89374b45-1f16-4726-bc32-f4159fef40e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'categories': ['course', \"Bachelor's\"],\n",
              " 'conversations': [['How long is the duration of the course?', 'Three years.'],\n",
              "  ['What are the entry requirements for the course?',\n",
              "   'Minimum entry requirements are a grade H5 and above in two higher level subjects together with a minimum of O6/H7 in four other subjects. A minimum of grade O6/H7 must be obtained in English. A grade O5/H6 must be obtained in Mathematics.For applicants whose first language is not English, please note the English language entry requirements. Mature applicants, applicants with a disability or those applying through the DARE or HEAR access schemes can find out more information on the application process.'],\n",
              "  ['How much is the tuition fees for the course?',\n",
              "   'The fees for this course for international students is €10000 per year. For domestic students applying through the CAO, this course applies under the free fees initiative.']]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset is split into question and answer lists. \n",
        "# For our chatbot, we have used the conversations subject of the dataset.\n",
        "\n",
        "questions, answers = [], []\n",
        "\n",
        "conversations = documents['conversations']\n",
        "\n",
        "for conv in conversations:\n",
        "  if len(conv) > 2 :\n",
        "    questions.append(conv[0])\n",
        "    replies = conv[1 :]\n",
        "    ans = ' '\n",
        "    for rep in replies:\n",
        "      ans += ' ' + rep\n",
        "      answers.append(ans)\n",
        "  elif len(conv) > 1:\n",
        "    questions.append(conv[0])\n",
        "    answers.append(conv[1])"
      ],
      "metadata": {
        "id": "0LbtFVWEtyio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions"
      ],
      "metadata": {
        "id": "jLf2Crt4vlUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f4daf39-1134-4e78-b802-54a3d1103183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How long is the duration of the course?',\n",
              " 'What are the entry requirements for the course?',\n",
              " 'How much is the tuition fees for the course?']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers"
      ],
      "metadata": {
        "id": "ItMtUiJwvuqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69ed6f7-2ae5-456b-9d82-a432b8197aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Three years.',\n",
              " 'Minimum entry requirements are a grade H5 and above in two higher level subjects together with a minimum of O6/H7 in four other subjects. A minimum of grade O6/H7 must be obtained in English. A grade O5/H6 must be obtained in Mathematics.For applicants whose first language is not English, please note the English language entry requirements. Mature applicants, applicants with a disability or those applying through the DARE or HEAR access schemes can find out more information on the application process.',\n",
              " 'The fees for this course for international students is €10000 per year. For domestic students applying through the CAO, this course applies under the free fees initiative.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing for seq2seq learning\n",
        "\n",
        "# For preprocessing, a single vocabulary is used for tokenization.\n",
        "\n",
        "answers_tags = []\n",
        "\n",
        "for i in range(len(answers)):\n",
        "  if type(answers[i]) == str:\n",
        "    answers_tags.append(answers[i])\n",
        "  else:\n",
        "    questions.pop(i)\n",
        "\n",
        "answers = []\n",
        "\n",
        "for i in range(len(answers_tags)):\n",
        "  answers.append('<START>' + answers_tags[i] + '<END>')\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "XPwIPbfdv5Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE \n",
        "\n",
        "# Calculates the VOCAB_SIZE variable by adding 1 to the length of the word index, \n",
        "# since the index starts from 1 and not 0. \n",
        "# This is the size of the vocabulary that will be used in the sequence-to-sequence mode"
      ],
      "metadata": {
        "id": "jb7n4GM8mG07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a808fc94-3f76-424a-d10d-e838a796ee10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = []\n",
        "\n",
        "for word in tokenizer.word_index:\n",
        "  vocab.append(word)\n",
        "\n",
        "def tokenize(sentences):\n",
        "  tokens_list = []\n",
        "  vocabulary = []\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    tokens = sentence.split()\n",
        "    vocabulary += tokens\n",
        "    tokens_list.append(tokens)\n",
        "  return tokens_list, vocabulary"
      ],
      "metadata": {
        "id": "x3HpSXfJmJ12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "xu7hIxr8v5qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder input data**: This is a 2D numpy array of shape (num_questions, maxlen_questions), where num_questions is the number of questions in the dataset and maxlen_questions is the maximum length of a question in the dataset. Each element of the array represents a word in a question and is an integer corresponding to the index of that word in the vocabulary. The encoder input data is fed into the encoder LSTM, which processes the entire input sequence and produces a context vector.\n",
        "\n",
        "\n",
        "\n",
        "**Decoder output data**: This is also a 2D numpy array of shape (num_answers, maxlen_answers), where each element represents a word in an answer and is an integer corresponding to the index of that word in the vocabulary. However, unlike the decoder input data, the decoder output data is shifted one time step to the right, so that the first element of each answer sequence is the second word of the original answer sequence, the second element is the third word, and so on. This is because the decoder LSTM is trained to predict the next word in the answer sequence based on the previous words, so the decoder output data serves as the \"expected output\" for the decoder LSTM. The decoder output data is also one-hot encoded using utils.to_categorical() function so that it can be used with the categorical cross-entropy loss function during training."
      ],
      "metadata": {
        "id": "Lb_oar3nrzQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder input data\n",
        "\n",
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "\n",
        "encoder_input_data = preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n",
        "\n",
        "#padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n",
        "\n",
        "#encoder_input_data = np.array([padded_questions])\n",
        "\n",
        "print(encoder_input_data.shape)"
      ],
      "metadata": {
        "id": "zh8_iiQFnOop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5c2eaf-545f-4f38-e7de-dc2d858b77c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder input data**: This is a 2D numpy array of shape (num_answers, maxlen_answers), where num_answers is the number of answers in the dataset and maxlen_answers is the maximum length of an answer in the dataset. Each element of the array represents a word in an answer and is an integer corresponding to the index of that word in the vocabulary. The decoder input data is fed into the decoder LSTM one word at a time, along with the context vector produced by the encoder LSTM."
      ],
      "metadata": {
        "id": "aqGmLGpNssPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder input data\n",
        "\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "\n",
        "decoder_input_data = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "\n",
        "#padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "\n",
        "#decoder_input_data = np.array(padded_answers)\n",
        "\n",
        "print(decoder_input_data.shape)"
      ],
      "metadata": {
        "id": "QnbZ5M8XpIOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ab6e8d-22d1-43d5-d97e-3dec202a8f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 87)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Decoder output data**: This is also a 2D numpy array of shape (num_answers, maxlen_answers), where each element represents a word in an answer and is an integer corresponding to the index of that word in the vocabulary. However, unlike the decoder input data, the decoder output data is shifted one time step to the right, so that the first element of each answer sequence is the second word of the original answer sequence, the second element is the third word, and so on. This is because the decoder LSTM is trained to predict the next word in the answer sequence based on the previous words, so the decoder output data serves as the \"expected output\" for the decoder LSTM. The decoder output data is also one-hot encoded using utils.to_categorical() function so that it can be used with the categorical cross-entropy loss function during training."
      ],
      "metadata": {
        "id": "8gVJeWdtsvOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder output data\n",
        "\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "for i in range(len(tokenized_answers)):\n",
        "  tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "\n",
        "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "\n",
        "decoder_output_data = utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
        "\n",
        "#onehot_answers = utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
        "\n",
        "#decoder_output_data = np.array([onehot_answers])\n",
        "\n",
        "print(decoder_output_data.shape)"
      ],
      "metadata": {
        "id": "L7k5PnDGpc39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5010aeaa-454d-4a66-b773-e6146050e486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 87, 81)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below lines of code denote a sequence-to-sequence model with an encoder and decoder architecture.\n",
        "\n",
        "encoder_inputs is an input layer that takes sequences of integers as inputs with shape (maxlen_questions,). This layer is used to feed the encoded question sequences to the model.\n",
        "\n",
        "encoder_embedding is an embedding layer that converts the integer input sequences to dense vectors of fixed size (batch_size, maxlen_questions, 200), where 200 is the size of the embedding dimension.\n",
        "\n",
        "encoder_outputs, state_h and state_c are output tensors from an LSTM layer with 200 units, which takes the embedded input sequences as input. encoder_outputs contains the output of the last timestep of the LSTM layer, while state_h and state_c represent the final cell state and hidden state of the LSTM layer. These states will be used as the initial states for the decoder LSTM.\n",
        "\n",
        "decoder_inputs is an input layer that takes sequences of integers as inputs with shape (maxlen_answers,). This layer is used to feed the encoded answer sequences to the model.\n",
        "\n",
        "decoder_embedding is an embedding layer that converts the integer input sequences to dense vectors of fixed size (batch_size, maxlen_answers, 200), where 200 is the size of the embedding dimension. This layer is used to convert the input answer sequences to dense vectors that can be used by the decoder LSTM.\n",
        "\n",
        "decoder_lstm is an LSTM layer with 200 units, which takes the embedded input sequences as input. The return_state and return_sequences parameters are set to True, which returns the output sequences and the final states of the LSTM layer. The final states of the LSTM layer will be used as the initial states for the decoder LSTM.\n",
        "\n",
        "decoder_outputs is an output tensor from the decoder LSTM layer. It contains the output sequence of the LSTM layer for each timestep.\n",
        "\n",
        "decoder_dense is a dense layer with VOCAB_SIZE units, which takes the output sequence of the decoder LSTM layer as input. It is used to convert the output sequence to a probability distribution over the output vocabulary.\n",
        "\n",
        "The final output of the model is obtained by passing the output sequence of the decoder LSTM layer through the dense layer.\n",
        "\n",
        "The model is compiled with the Adam optimizer, categorical cross-entropy loss function, and accuracy metric. The model is trained to minimize the categorical cross-entropy loss between the predicted and actual output sequences."
      ],
      "metadata": {
        "id": "Jr7rieXLtRqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras Functional API is used to build the architecture of the model. \n",
        "# The model is a multi input model, the encoder input and the decoder input. \n",
        "# Successive layers include the Embedding and the LSTM layers\n",
        "\n",
        "# Embedding LSTM and Desne Layers\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions, ))\n",
        "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(200, return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers, ))\n",
        "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM(200, return_state=True, return_sequences=True)\n",
        "decoder_outputs, _ , _  = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax)\n",
        "output = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "model.compile(optimizer='adam', loss= tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "JzxkC4EGqWv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "8l16Z_OUt_iG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96ca822-717b-43c9-f51f-72e5dbfc1944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 9)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 87)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 9, 200)       16200       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 87, 200)      16200       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 200),        320800      ['embedding[0][0]']              \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 87, 200),    320800      ['embedding_1[0][0]',            \n",
            "                                 (None, 200),                     'lstm[0][1]',                   \n",
            "                                 (None, 200)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 87, 81)       16281       ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 690,281\n",
            "Trainable params: 690,281\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below lines of code trains a neural machine translation model by fitting it to the input data. The input data consists of three parts:\n",
        "\n",
        "encoder_input_data: This is a 2D array that represents the input sequences for the encoder. Each row in the array represents a single input sequence, and the columns represent the tokens in the sequence. The shape of the array is (num_sequences, maxlen_questions).\n",
        "\n",
        "decoder_input_data: This is a 2D array that represents the input sequences for the decoder. Each row in the array represents a single input sequence, and the columns represent the tokens in the sequence. The shape of the array is (num_sequences, maxlen_answers).\n",
        "\n",
        "decoder_output_data: This is a 3D array that represents the output sequences for the decoder. Each row in the array represents a single output sequence, and the columns represent the tokens in the sequence. The shape of the array is (num_sequences, maxlen_answers, VOCAB_SIZE).\n",
        "\n",
        "The model.fit function trains the neural machine translation model using the input data. The batch_size parameter specifies the number of input sequences to process at once, and the epochs parameter specifies the number of times to iterate over the entire input data during training."
      ],
      "metadata": {
        "id": "DAkmxOfYttHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=32, epochs=100)"
      ],
      "metadata": {
        "id": "8qCp44nYuJrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a897f53-c8e4-415a-eb5d-cbe7db8eed7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 10s 10s/step - loss: 4.3952 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 4.3855 - accuracy: 0.0583\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 4.3755 - accuracy: 0.1500\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 4.3648 - accuracy: 0.1583\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 4.3527 - accuracy: 0.1833\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 4.3385 - accuracy: 0.1917\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 4.3203 - accuracy: 0.2000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 4.2947 - accuracy: 0.1833\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 4.2535 - accuracy: 0.1750\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 4.1951 - accuracy: 0.1583\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 4.1372 - accuracy: 0.1083\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 4.0845 - accuracy: 0.0667\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 4.0439 - accuracy: 0.0583\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 3.9916 - accuracy: 0.0750\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 3.9127 - accuracy: 0.0833\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 3.8580 - accuracy: 0.0750\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 3.7572 - accuracy: 0.1083\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 3.6914 - accuracy: 0.0750\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 3.6089 - accuracy: 0.1000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 3.5270 - accuracy: 0.1083\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 3.4749 - accuracy: 0.1000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 3.4047 - accuracy: 0.1000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 3.3420 - accuracy: 0.1250\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 3.2697 - accuracy: 0.1167\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 3.2132 - accuracy: 0.1083\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 3.1404 - accuracy: 0.1333\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 3.0852 - accuracy: 0.1583\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 3.0158 - accuracy: 0.1583\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 2.9575 - accuracy: 0.1333\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 2.9169 - accuracy: 0.1667\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 2.8664 - accuracy: 0.1667\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 2.7866 - accuracy: 0.1833\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 2.7557 - accuracy: 0.1833\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 2.7049 - accuracy: 0.1750\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 2.6382 - accuracy: 0.1833\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 2.6091 - accuracy: 0.2167\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 2.5432 - accuracy: 0.2417\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 2.4978 - accuracy: 0.2667\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 2.4677 - accuracy: 0.3000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 2.4042 - accuracy: 0.2583\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 2.3632 - accuracy: 0.2667\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 2.3384 - accuracy: 0.2583\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 2.2831 - accuracy: 0.3083\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 2.2331 - accuracy: 0.3500\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 2.2036 - accuracy: 0.3583\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 2.1772 - accuracy: 0.2750\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 2.1451 - accuracy: 0.3417\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 2.0927 - accuracy: 0.3583\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 2.0525 - accuracy: 0.4000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 2.0321 - accuracy: 0.4083\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 2.0116 - accuracy: 0.3833\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 1.9800 - accuracy: 0.4583\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 1.9311 - accuracy: 0.5167\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 1.9005 - accuracy: 0.5250\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 1.8876 - accuracy: 0.4667\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 1.8637 - accuracy: 0.5083\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 1.8278 - accuracy: 0.5167\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 1.7923 - accuracy: 0.5833\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 1.7748 - accuracy: 0.5917\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 1.7623 - accuracy: 0.5667\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 1.7328 - accuracy: 0.5833\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 1.6988 - accuracy: 0.6583\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 1.6742 - accuracy: 0.6833\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 1.6596 - accuracy: 0.6417\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 1.6462 - accuracy: 0.6417\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 1.6225 - accuracy: 0.6667\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 1.5932 - accuracy: 0.7333\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 1.5666 - accuracy: 0.7750\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 1.5486 - accuracy: 0.7750\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 1.5354 - accuracy: 0.8083\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 1.5202 - accuracy: 0.8083\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 1.5004 - accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 1.4736 - accuracy: 0.8667\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 1.4486 - accuracy: 0.8917\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 1.4292 - accuracy: 0.9000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 1.4141 - accuracy: 0.8833\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 1.4012 - accuracy: 0.8917\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 1.3857 - accuracy: 0.8750\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 1.3658 - accuracy: 0.9167\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 1.3420 - accuracy: 0.9167\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 1.3195 - accuracy: 0.9417\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 1.3015 - accuracy: 0.9500\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 1.2867 - accuracy: 0.9500\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 1.2729 - accuracy: 0.9500\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 1.2565 - accuracy: 0.9500\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 1.2373 - accuracy: 0.9500\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 1.2161 - accuracy: 0.9500\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 1.1961 - accuracy: 0.9500\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 1.1790 - accuracy: 0.9583\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 1.1637 - accuracy: 0.9667\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 1.1489 - accuracy: 0.9583\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 1.1330 - accuracy: 0.9667\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 1.1153 - accuracy: 0.9750\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 1.0965 - accuracy: 0.9750\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 1.0777 - accuracy: 0.9833\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 1.0603 - accuracy: 0.9833\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 1.0442 - accuracy: 0.9833\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 1.0287 - accuracy: 0.9917\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 194ms/step - loss: 1.0132 - accuracy: 0.9833\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.9968 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb0b534a350>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference in LSTM (Long Short-Term Memory) refers to the process of generating predictions using a trained LSTM model on new, unseen data.\n",
        "\n",
        "The inference() function creates two models, the encoder model and the decoder model, which will be used to generate responses to questions based on the trained neural network. The encoder model takes in the input sequence of the question and generates the encoder states, which will be used to generate the response. The decoder model takes in the decoder input sequence along with the decoder states and generates the decoder output sequence and the decoder states."
      ],
      "metadata": {
        "id": "s2N7OwxauTJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making inferences\n",
        "\n",
        "# For making inferences, two inference models namely the encoder and the decoder inference model are built. \n",
        "# These models undergo similar preprocessing steps as the model did during the training phase.\n",
        "\n",
        "def inference():\n",
        "  encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "  decoder_state_input_h = tf.keras.layers.Input(shape=(200 ,))\n",
        "  decoder_state_input_c = tf.keras.layers.Input(shape=(200 ,))\n",
        "    \n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "  decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "  decoder_embedding , initial_state=decoder_states_inputs)\n",
        "  decoder_states = [state_h, state_c]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = tf.keras.models.Model(\n",
        "      [decoder_inputs] + decoder_states_inputs,\n",
        "      [decoder_outputs] + decoder_states)\n",
        "    \n",
        "  return encoder_model , decoder_model"
      ],
      "metadata": {
        "id": "doRqooQ815Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preprocess_input() function takes in an input sentence and tokenizes it, converts the tokens to their corresponding word indices, and then pads the sequence to ensure it has the same length as the maximum length of the input sequences used in training the neural network. The output of this function is the padded token sequence."
      ],
      "metadata": {
        "id": "XwDnZ8u_ua3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input(input_sentence):\n",
        "    tokens = input_sentence.lower().split()\n",
        "    tokens_list = []\n",
        "    for word in tokens:\n",
        "        tokens_list.append(tokenizer.word_index[word]) \n",
        "    return preprocessing.sequence.pad_sequences([tokens_list] , maxlen=maxlen_questions , padding='post')"
      ],
      "metadata": {
        "id": "U9i0EQh72fz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final two lines of code call the inference() function to obtain the encoder and decoder models and store them in enc_model and dec_model, respectively."
      ],
      "metadata": {
        "id": "nffrMqsaudXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_model , dec_model = inference()"
      ],
      "metadata": {
        "id": "1YBNAqaO2hJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot Testing"
      ],
      "metadata": {
        "id": "P96FOaPWwVxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference is being performed correctly by first encoding the input query using the trained encoder model, and then decoding it using the trained decoder model by iteratively predicting the next word until the 'end' token is generated or the maximum length of the answer sequence is reached. The predicted output is then printed as the bot's response to the input query."
      ],
      "metadata": {
        "id": "b10zNMCsuxlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tests = ['How long is the duration of the course', 'What are the entry requirements for the course', 'How much is the tuition fees for the course']\n",
        "\n",
        "for i in range(3):\n",
        "    states_values = enc_model.predict(preprocess_input(tests[i]))\n",
        "    empty_target_seq = np.zeros((1 , 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    \n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        \n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += f' {word}'\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros((1 , 1))  \n",
        "        empty_target_seq[0 , 0] = sampled_word_index\n",
        "        states_values = [h , c] \n",
        "    print(f'Human: {tests[i]}')\n",
        "    print()\n",
        "    decoded_translation = decoded_translation.split(' end')[0]\n",
        "    print(f'Bot: {decoded_translation}')\n",
        "    print('-'*25)\n"
      ],
      "metadata": {
        "id": "xMyah40_2kTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9447a6e2-174e-46ff-8e41-05713fa34bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Human: How long is the duration of the course\n",
            "\n",
            "Bot:  three years\n",
            "-------------------------\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Human: What are the entry requirements for the course\n",
            "\n",
            "Bot:  minimum entry requirements are a grade h5 and above in two higher level subjects together with a minimum of o6 h7 in four other subjects a minimum of grade o6 h7 must be obtained in english a grade o5 h6 must be obtained in mathematics for applicants whose first language is not english please note the english language entry requirements applicants applicants applicants with a disability or those applying through the dare or hear access schemes can find out more information on the application process\n",
            "-------------------------\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Human: How much is the tuition fees for the course\n",
            "\n",
            "Bot:  the fees for this course for international students is €10000 per year for domestic students applying through the cao this course applies under the free fees initiative\n",
            "-------------------------\n"
          ]
        }
      ]
    }
  ]
}